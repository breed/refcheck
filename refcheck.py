# This script extracts references from a PDF file and checks their validity.
# It was initially generated by copilot but modified quite a bit to actually work.
import os
from os.path import isdir

import fitz  # PyMuPDF
import re
import requests

DOI_ORG_PREFIX = "https://doi.org/"

DOI_ORG_API = "https://doi.org/api/handles/"

OPENALEX_API = "https://api.openalex.org/works"


def extract_text_from_pdf(pdf_path):
    doc = fitz.open(pdf_path)
    text = ""
    for page in doc:
        text += page.get_text()
    return text

URL_PATTERN = re.compile(r'https?://\S+$')


def extract_references(text):
    # Roughly extract references section
    references_section = re.split(r'references|bibliography', text, flags=re.IGNORECASE)[-1]
    ref = None
    for line in references_section.splitlines():
        if line.startswith("["):
            if ref:
                yield ref
            ref = line
            continue
        if ref:
            # if we have a line break in the middle of a URL we don't want to add a space
            if URL_PATTERN.search(ref):
                ref += line
            else:
                # fix any hyphenated lines
                if ref.endswith("-"):
                    ref = ref[:-1] + line
                else:
                    ref += " " + line
    if ref:
        yield ref
#    ref_candidates = re.split(r'\n\d+\.\s+|\n(?=\[?\d+\]?\s+)', references_section)
 #   references = [ref.strip() for ref in ref_candidates if len(ref.strip()) > 10]
  #  return references


def find_urls_or_dois(ref):
    urls = re.findall(r'https?://\S+|www\.\S+', ref)
    dois = re.findall(r'10\.\d{4,9}/[-._;()/:A-Z0-9]+', ref, flags=re.IGNORECASE)
    # Remove trailing periods from URLs
    return [url.rstrip('.') for url in urls + ["https://doi.org/" + doi for doi in dois]]

def check_url_validity(url):
    try:
        if url.startswith(DOI_ORG_PREFIX):
            url = DOI_ORG_API + url[len(DOI_ORG_PREFIX):]
        response = requests.head(url, allow_redirects=True, timeout=10)
        return response.status_code < 400
    except requests.RequestException:
        return False


def search_openalex(title):
    try:
        response = requests.get(OPENALEX_API, params={"search": title, "per-page": 1}, timeout=10)
        if response.status_code == 200:
            data = response.json()
            return data
    except requests.RequestException:
        return None
    return None


def normalize_quotes(ref: str) -> str:
    # Define a dictionary of different double quote characters to replace
    quote_replacements = {
        '“': '"',
        '”': '"',
        '„': '"',
        '‟': '"',
        '«': '"',
        '»': '"'
    }

    # Replace each quote character in the dictionary with the standard double quote
    for old_quote, new_quote in quote_replacements.items():
        ref = ref.replace(old_quote, new_quote)
    return ref

def extract_possible_title(ref):
    # Heuristic: look for quoted string or title-like portion

    ref = normalize_quotes(ref)
    quoted = re.findall(r'"([^"]+)"', ref)
    if quoted:
        return quoted[0]

    # Fallback: split by period the 2nd part (should be after the authors)
    parts = ref.split('. ')

    return parts[1] if len(parts) > 1 else ref


def check_references_validity(references):
    results = []
    for ref in references:
        links = find_urls_or_dois(ref)
        result = ""
        if links:
            valid = all(check_url_validity(url) for url in links)
            result += str(links) + ("Valid (link)" if valid else "Invalid (link)") + "\n"
        title = extract_possible_title(ref)
        result += "Checking Alex for: " + title + "\n"
        alex_ref = search_openalex(title)
        if alex_ref and alex_ref['meta']['count'] > 0:
            for item in alex_ref['results']:
                result += f"Alex: "
                result += ",".join([x['author']['display_name'] for x in item['authorships']]) + ', '
                result += item['title'] + ", " + item["publication_date"] + ", "
                if "locations" not in item:
                    result += "No locations\n"
                else:
                    for location in item["locations"]:
                        if "is_published" not in location or not location["is_published"]:
                            result += "not published" + ";"
                        if "source" in location and location["source"]:
                            result += location["source"]["display_name"] + ";"
                        else:
                            result += "No source" + ";"
        else:
            result += "No OpenAlex match"
        results.append((ref, result))
    return results


def main(pdf_path):
    print(f"{pdf_path} exists {os.path.exists(pdf_path)} and is a directory {isdir(pdf_path)}")
    if isdir(pdf_path):
        for root, dirs, files in os.walk(pdf_path):
            for file in [os.path.join(root, f) for f in files if f.endswith('.pdf')]:
                check_references(file)
                print("-----------------------------\n")
    else:
        check_references(pdf_path)


def check_references(pdf_path):
    print(f"Extracting references from: {pdf_path}")
    text = extract_text_from_pdf(pdf_path)
    references = [x for x in extract_references(text)]
    print(f"Found {len(references)} references.\n")
    results = check_references_validity(references)

    for (ref, status) in results:
        print("Citation: " + ref)
        print("Search result: " + status)


if __name__ == "__main__":
    import sys

    if len(sys.argv) != 2:
        print("Usage: python check_pdf_references.py <path_to_pdf>")
    else:
        main(sys.argv[1])
